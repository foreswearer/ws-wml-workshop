{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Introduction to Spark lab, part 3: machine learning\n\nIn this notebook you'll learn how to create a model for purchase recommendations using the alternating least squares algorithm of the Spark machine learning library. Machine learning is based on algorithms that can learn from data without relying on rules-based programming.  \n\n\"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E\"\n-Tom M. Mitchell\n\nThis notebook uses pySpark, the Python API for Spark. Some knowledge of Python is recommended. This notebook runs on Python 3.6 with Spark.\n\nIf you are new to Spark, see the first two parts of this lab: \n - <a href=\"https://dataplatform.cloud.ibm.com/exchange/public/entry/view/95811fca38af4ccbea8acf8658bedcfc\" target=\"_blank\" rel=\"noopener noreferrer\">Introduction to Spark lab, part 1: Basic Concepts</a>\n - <a href=\"https://dataplatform.cloud.ibm.com/exchange/public/entry/view/5ad1c820f57809ddec9a040e37b2bd55\" target=\"_blank\" rel=\"noopener noreferrer\">Introduction to Spark lab, part 2: Querying data</a>\n\n## Spark machine learning library\nThe Spark machine learning library makes practical machine learning scalable and easy. The library consists of common machine learning algorithms and utilities, including classification, regression, clustering, collaborative filtering (this notebook!), dimensionality reduction, lower-level optimization primitives, and higher-level pipeline APIs.\n\nThe library has two packages:\n- spark.mllib contains the original API that handles data in RDDs. It's in maintenance mode, but fully supported.\n- spark.ml contains a newer API for constructing ML pipelines. It handles data in DataFrames. It's being actively enhanced.\n\n## Alternating least squares algorithm\nThe alternating least squares (ALS) algorithm provides collaborative filtering between customers and products to find products that the customers might like, based on their previous purchases or ratings.\n\nThe ALS algorithm creates a matrix of all customers versus all products. Most cells in the matrix are empty, which means the customer hasn't bought that product. The ALS algorithm then fills in the probability of customers buying products that they haven't bought yet, based on similarities between customer purchases and similarities between products. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between fixing the customer factors and solving for product factors and fixing the product factors and solving for customer factors.\n\nYou don't, however, need to understand how the ALS algorithm works to use it! Spark machine learning algorithms have default values that work well in most cases.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Table of contents\n\n1. [Get the data](#getdata)<br>\n2. [Prepare and shape the data](#prepare)<br>\n    2.1 [Format the data](#prepare1)<br>\n    2.2 [Clean the data](#prepare2)<br>\n    2.3 [Create a DataFrame](#prepare3)<br>\n    2.4 [Remove unneeded columns](#prepare4)<br>\n3. [Split the data into three sets](#split)<br>\n4. [Build recommendation models](#model)<br>\n5. [Test the models](#test)<br>\n    5.1 [Clean the cross validation data set](#test1)<br>\n    5.2 [Run the models on the cross validation data set](#test2)<br>\n    5.3 [Calculate the accuracy for each model](#test3)<br>\n    5.4 [Confirm the best model](#test4)<br>\n6. [Implement the mode](#implement)<br>\n    6.1 [Create a DataFrame for the customer and all products](#implement1)<br>\n    6.2 [Rate each product](#implement2)<br>\n    6.3 [Find the top recommendations](#implement3)<br>\n    6.4 [Compare purchased and recommended products](#implement4)<br>\n7. [Summary and next steps](#summary)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"getdata\"></a>\n## 1. Get the data \nThe data set contains the transactions of an online retailer of gift items for the period from 01/12/2010 to 09/12/2011. Many of the customers are wholesalers.\n\nYou'll be using a slightly modified version of UCI's <a href=\"http://archive.ics.uci.edu/ml/datasets/Online+Retail\" target=\"_blank\" rel=\"noopener noreferrer\">Online Retail Data Set</a>.  \n\nHere's a glimpse of the data:\n\n<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/FullFile.png' width=\"80%\" height=\"80%\"></img>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Download the CSV version of the data set, from which commas in the product descriptions are removed:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!rm 'OnlineRetail.csv.gz' -f\n!wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190705090704-0000\nKERNEL_ID = 969bfab2-80ae-48d9-adf5-07f429f86124\n--2019-07-05 09:07:06--  https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7483128 (7.1M) [application/octet-stream]\nSaving to: 'OnlineRetail.csv.gz'\n\nOnlineRetail.csv.gz 100%[===================>]   7.14M  --.-KB/s    in 0.1s    \n\n2019-07-05 09:07:07 (52.3 MB/s) - 'OnlineRetail.csv.gz' saved [7483128/7483128]\n\n"
                }
            ], 
            "execution_count": 1
        }, 
        {
            "source": "Put the data into an RDD and print the first 5 rows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "loadRetailData = sc.textFile(\"OnlineRetail.csv.gz\")\n\nfor row in loadRetailData.take(5):\n    print (row)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/10 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/10 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/10 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/10 8:26,3.39,17850,United Kingdom\n"
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "Each row in the RDD is a string that correlates to a line in the CSV file.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"prepare\"></a>\n## 2. Prepare and shape the data\n\nIt's been said that preparing and shaping data is 80% of a data scientist's job. Having the right data in the right format is critical for getting accurate results.\n\nTo get the data ready, complete these tasks:\n\n1. [Format the data](#prepare1)\n1. [Clean the data](#prepare2)\n1. [Create a DataFrame](#prepare3)\n1. [Remove unneeded columns](#prepare4)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"prepare1\"></a>\n### 2.1 Format the data\nRemove the header from the RDD and split the string in each row with a comma:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "header = loadRetailData.first()\nloadRetailData = loadRetailData.filter(lambda line: line != header).\\\n                            map(lambda l: l.split(\",\"))\n\nfor row in loadRetailData.take(5):\n    print (row)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['536365', '85123A', 'WHITE HANGING HEART T-LIGHT HOLDER', '6', '12/1/10 8:26', '2.55', '17850', 'United Kingdom']\n['536365', '71053', 'WHITE METAL LANTERN', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']\n['536365', '84406B', 'CREAM CUPID HEARTS COAT HANGER', '8', '12/1/10 8:26', '2.75', '17850', 'United Kingdom']\n['536365', '84029G', 'KNITTED UNION FLAG HOT WATER BOTTLE', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']\n['536365', '84029E', 'RED WOOLLY HOTTIE WHITE HEART.', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']\n"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "source": "<a id=\"prepare2\"></a>\n### 2.2 Clean the data\nRemove the rows that have incomplete data. Keep only the rows that meet the following criteria:\n - The purchase quantity is greater than 0 \n - The customer ID not equal to 0 \n - The stock code is not blank after you remove non-numeric characters", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import re\n\nloadRetailData = loadRetailData.filter(lambda l: int(l[3]) > 0\\\n                                and len(re.sub(\"\\D\", \"\", l[1])) != 0 \\\n                                and len(l[6]) != 0)\n\nprint (loadRetailData.take(2))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[['536365', '85123A', 'WHITE HANGING HEART T-LIGHT HOLDER', '6', '12/1/10 8:26', '2.55', '17850', 'United Kingdom'], ['536365', '71053', 'WHITE METAL LANTERN', '6', '12/1/10 8:26', '3.39', '17850', 'United Kingdom']]\n"
                }
            ], 
            "execution_count": 4
        }, 
        {
            "source": "<a id=\"prepare3\"></a>\n### 2.3 Create a DataFrame\n\nFirst, create an SQLContext and map each line to a row: ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql import SQLContext, Row\nsqlContext = SQLContext(sc)\n\n#Convert each line to a Row.\nloadRetailData = loadRetailData.map(lambda l: Row(inv=int(l[0]),\\\n                                    stockCode=int(re.sub(\"\\D\", \"\", l[1])),\\\n                                    description=l[2],\\\n                                    quant=int(l[3]),\\\n                                    invDate=l[4],\\\n                                    price=float(l[5]),\\\n                                    custId=int(l[6]),\\\n                                    country=l[7]))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "Create a DataFrame and show the inferred schema:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "retailDf = sqlContext.createDataFrame(loadRetailData)\nprint (retailDf.printSchema())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "root\n |-- country: string (nullable = true)\n |-- custId: long (nullable = true)\n |-- description: string (nullable = true)\n |-- inv: long (nullable = true)\n |-- invDate: string (nullable = true)\n |-- price: double (nullable = true)\n |-- quant: long (nullable = true)\n |-- stockCode: long (nullable = true)\n\nNone\n"
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "Register the DataFrame as a table so that you can run SQL queries on it and show the first two rows:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "retailDf.registerTempTable(\"retailPurchases\")\nsqlContext.sql(\"SELECT * FROM retailPurchases limit 2\").toPandas()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>custId</th>\n      <th>description</th>\n      <th>inv</th>\n      <th>invDate</th>\n      <th>price</th>\n      <th>quant</th>\n      <th>stockCode</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>United Kingdom</td>\n      <td>17850</td>\n      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n      <td>536365</td>\n      <td>12/1/10 8:26</td>\n      <td>2.55</td>\n      <td>6</td>\n      <td>85123</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>United Kingdom</td>\n      <td>17850</td>\n      <td>WHITE METAL LANTERN</td>\n      <td>536365</td>\n      <td>12/1/10 8:26</td>\n      <td>3.39</td>\n      <td>6</td>\n      <td>71053</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "          country  custId                         description     inv  \\\n0  United Kingdom   17850  WHITE HANGING HEART T-LIGHT HOLDER  536365   \n1  United Kingdom   17850                 WHITE METAL LANTERN  536365   \n\n        invDate  price  quant  stockCode  \n0  12/1/10 8:26   2.55      6      85123  \n1  12/1/10 8:26   3.39      6      71053  "
                    }, 
                    "execution_count": 7, 
                    "metadata": {}
                }
            ], 
            "execution_count": 7
        }, 
        {
            "source": "<a id=\"prepare4\"></a>\n### 2.4 Remove unneeded columns\nThe only columns you need are `custId`, `stockCode`, and a new column, `purch`, which has a value of 1 to indicate that the customer purchased the product:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "query = \"\"\"\nSELECT \n    custId, stockCode, 1 as purch\nFROM \n    retailPurchases \ngroup \n    by custId, stockCode\"\"\"\nretailDf = sqlContext.sql(query)\nretailDf.registerTempTable(\"retailDf\")\n\nsqlContext.sql(\"select * from retailDf limit 10\").toPandas()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>custId</th>\n      <th>stockCode</th>\n      <th>purch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18074</td>\n      <td>22224</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13705</td>\n      <td>21889</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15862</td>\n      <td>22441</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15862</td>\n      <td>21592</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12838</td>\n      <td>22739</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12838</td>\n      <td>22149</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>14078</td>\n      <td>22548</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>14078</td>\n      <td>22423</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>12433</td>\n      <td>21977</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>14696</td>\n      <td>84360</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   custId  stockCode  purch\n0   18074      22224      1\n1   13705      21889      1\n2   15862      22441      1\n3   15862      21592      1\n4   12838      22739      1\n5   12838      22149      1\n6   14078      22548      1\n7   14078      22423      1\n8   12433      21977      1\n9   14696      84360      1"
                    }, 
                    "execution_count": 8, 
                    "metadata": {}
                }
            ], 
            "execution_count": 8
        }, 
        {
            "source": "<a id=\"split\"></a>\n## 3. Split the data into three sets\nYou'll split the data into three sets: \n - a testing data set (10% of the data)\n - a cross-validation data set (10% of the data)\n - a training data set (80% of the data)\n\nSplit the data randomly and create a DataFrame for each data set:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "testDf, cvDf, trainDf = retailDf.randomSplit([.1,.1,.8],1)\n\nprint (\"trainDf count: \", trainDf.count(), \" example: \")\nfor row in trainDf.take(2): print (row)\nprint ()\n\nprint (\"cvDf count: \", cvDf.count(), \" example: \")\nfor row in cvDf.take(2): print (row)\nprint ()\n\nprint (\"testDf count: \", testDf.count(), \" example: \")\nfor row in testDf.take(2): print (row)\nprint ()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "trainDf count:  208123  example: \nRow(custId=12359, stockCode=23345, purch=1)\nRow(custId=12363, stockCode=20685, purch=1)\n\ncvDf count:  25876  example: \nRow(custId=12349, stockCode=23545, purch=1)\nRow(custId=12388, stockCode=22960, purch=1)\n\ntestDf count:  26113  example: \nRow(custId=12362, stockCode=22372, purch=1)\nRow(custId=12391, stockCode=20985, purch=1)\n\n"
                }
            ], 
            "execution_count": 9
        }, 
        {
            "source": "<a id=\"model\"></a>\n## 4. Build recommendation models\nMachine learning algorithms have standard parameters and hyperparameters. Standard parameters specify data and options. Hyperparameters control the performance of the algorithm.\n\nThe ALS algorithm has these hyperparameters:  \n\n - The `rank` hyperparameter represents the number of features. The default value of `rank` is 10.\n - The `maxIter` hyperparameter represents the number of iterations to run the least squares computation. The default value of `maxIter` is 10.\n\nUse the training DataFrame to train three models with the ALS algorithm with different values for the `rank` and `maxIter` hyperparameters. Assign the `userCol`, `itemCol`, and `ratingCol` parameters to the appropriate data columns. Set the `implicitPrefs` parameter to `true` so that the algorithm can predict latent factors.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.recommendation import ALS\n\nals1 = ALS(rank=3, maxIter=15,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel1 = als1.fit(trainDf)\n\nals2 = ALS(rank=15, maxIter=3,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel2 = als2.fit(trainDf)\n\nals3 = ALS(rank=15, maxIter=15,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel3 = als3.fit(trainDf)\n\nprint (\"The models are trained\")", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "The models are trained\n"
                }
            ], 
            "execution_count": 10
        }, 
        {
            "source": "<a id=\"test\"></a>\n## 5. Test the models\n\nFirst, test the three models on the cross-validation data set, and then on the testing data set. \n\nYou'll know the model is accurate when the prediction values for products that the customers have already bought are close to 1. \n\n<a id=\"test1\"></a>\n### 5.1 Clean the cross validation data set\n\nRemove any of the customers or products in the cross-validation data set that are not in the training data set:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import BooleanType\ncustomers = set(trainDf.rdd.map(lambda line: line.custId).collect())\nstock = set(trainDf.rdd.map(lambda line: line.stockCode).collect())\n\nprint (cvDf.count())\ncvDf = cvDf.rdd.filter(lambda line: line.stockCode in stock and\\\n                                           line.custId in customers).toDF()\nprint (cvDf.count())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "25876\n25846\n"
                }
            ], 
            "execution_count": 11
        }, 
        {
            "source": "<a id=\"test2\"></a>\n### 5.2 Run the models on the cross-validation data set\nRun the model with the cross-validation DataFrame by using the `transform` function and print the first two rows of each set of predictions:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "predictions1 = model1.transform(cvDf)\npredictions2 = model2.transform(cvDf)\npredictions3 = model3.transform(cvDf)\n\nprint (predictions1.take(2))\nprint (predictions2.take(2))\nprint (predictions3.take(2))", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Row(custId=14606, stockCode=20735, purch=1, prediction=0.027699407190084457), Row(custId=16464, stockCode=20735, purch=1, prediction=0.009662602096796036)]\n[Row(custId=14606, stockCode=20735, purch=1, prediction=0.04637198522686958), Row(custId=16464, stockCode=20735, purch=1, prediction=0.004486199002712965)]\n[Row(custId=14606, stockCode=20735, purch=1, prediction=0.09784500300884247), Row(custId=16464, stockCode=20735, purch=1, prediction=0.005791308358311653)]\n"
                }
            ], 
            "execution_count": 12
        }, 
        {
            "source": "<a id=\"test3\"></a>\n### 5.3 Calculate the accuracy for each model  \n\nYou'll use the mean squared error calculation to determine accuracy by comparing the prediction values for products to the actual purchase values. Remember that if a customer purchased a product, the value in the `purch` column is 1. The mean squared error calculation measures the average of the squares of the errors between what is estimated and the existing data. The lower the mean squared error value, the more accurate the model. \n\nFor all predictions, subtract the prediction from the actual purchase value (1), square the result, and calculate the mean of all of the squared differences:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "meanSquaredError1 = predictions1.rdd.map(lambda line: (line.purch - line.prediction)**2).mean()\nmeanSquaredError2 = predictions2.rdd.map(lambda line: (line.purch - line.prediction)**2).mean()\nmeanSquaredError3 = predictions3.rdd.map(lambda line: (line.purch - line.prediction)**2).mean()\n    \nprint ('Mean squared error = %.4f for our first model' % meanSquaredError1)\nprint ('Mean squared error = %.4f for our second model' % meanSquaredError2)\nprint ('Mean squared error = %.4f for our third model' % meanSquaredError3)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Mean squared error = 0.7397 for our first model\nMean squared error = 0.6944 for our second model\nMean squared error = 0.6682 for our third model\n"
                }
            ], 
            "execution_count": 13
        }, 
        {
            "source": "The third model (model3) has the lowest mean squared error value, so it's the most accurate.\n\nNotice that of the three models, model3 has the highest values for the hyperparameters. At this point you might be tempted to run the model with even higher values for `rank` and `maxIter`. However, you might not get better results. Increasing the values of the hyperparameters increases the time for the model to run. Also, you don't want to overfit the model so that it exactly fits the original data. In that case, you wouldn't get any recommendations! For best results, keep the values of the hyperparameters close to the defaults.\n\n<a id=\"test4\"></a>\n### 5.4 Confirm the best model \n\nNow run model3 on the testing data set to confirm that it's the best model. You want to make sure that the model is not over-matched to the cross-validation data. It's possible for a model to match one subset of the data well but not another. If the values of the mean squared error for the testing data set and the cross-validation data set are close, then you've confirmed that the model works for all the data.\n\nClean the testing data set, run model3 on the testing data set, and calculate the mean squared error:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "filteredTestDf = testDf.rdd.filter(lambda line: line.stockCode in stock and\\\n                                              line.custId in customers).toDF()\npredictions4 = model3.transform(filteredTestDf)\nmeanSquaredError4 = predictions4.rdd.map(lambda line: (line.purch - line.prediction)**2).mean()\n    \nprint ('Mean squared error = %.4f for our best model' % meanSquaredError4)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Mean squared error = 0.6695 for our best model\n"
                }
            ], 
            "execution_count": 14
        }, 
        {
            "source": "That's pretty close. The model works for all the data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"implement\"></a>\n## 6. Implement the model\n\nUse the best model to predict which products a specific customer might be interested in purchasing.\n\n<a id=\"implement1\"></a>\n### 6.1 Create a DataFrame for the customer and all products \n\nCreate a DataFrame in which each row has the customer ID (15544) and a product ID:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.sql.functions import lit\n\nstock15544 = set(trainDf.filter(trainDf['custId'] == 15544).rdd.map(lambda line: line.stockCode).collect())\n\nuserItems = trainDf.select(\"stockCode\").distinct().\\\n            withColumn('custId', lit(15544)).\\\n            rdd.filter(lambda line: line.stockCode not in stock15544).toDF()\n\nfor row in userItems.take(5):\n    print (row.stockCode, row.custId)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "21899 15544\n22429 15544\n22201 15544\n22165 15544\n21209 15544\n"
                }
            ], 
            "execution_count": 15
        }, 
        {
            "source": "<a id=\"implement2\"></a>\n### 6.2 Rate each product\n\nRun the `transform` function to create a prediction value for each product:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "userItems = model3.transform(userItems)\n\nfor row in userItems.take(5):\n    print (row.stockCode, row.custId, row.prediction)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "20735 15544 0.010625979863107204\n21220 15544 0.06406569480895996\n21700 15544 0.06700766086578369\n22097 15544 -0.06914416700601578\n22223 15544 0.024163667112588882\n"
                }
            ], 
            "execution_count": 16
        }, 
        {
            "source": "<a id=\"implement3\"></a>\n### 6.3 Find the top recommendations\n\nPrint the top five product recommendations:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "userItems.registerTempTable(\"predictions\")\nquery = \"select * from predictions order by prediction desc limit 5\"\n\nsqlContext.sql(query).toPandas()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stockCode</th>\n      <th>custId</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21242</td>\n      <td>15544</td>\n      <td>0.537044</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22090</td>\n      <td>15544</td>\n      <td>0.519475</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22326</td>\n      <td>15544</td>\n      <td>0.509296</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21987</td>\n      <td>15544</td>\n      <td>0.494924</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21122</td>\n      <td>15544</td>\n      <td>0.487287</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   stockCode  custId  prediction\n0      21242   15544    0.537044\n1      22090   15544    0.519475\n2      22326   15544    0.509296\n3      21987   15544    0.494924\n4      21122   15544    0.487287"
                    }, 
                    "execution_count": 17, 
                    "metadata": {}
                }
            ], 
            "execution_count": 17
        }, 
        {
            "source": "<a id=\"implement4\"></a>\n### 6.4 Compare purchased and recommended products\n\nHere's a view of the products that customer 15544 bought:\n\n<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/user.png' width=\"80%\" height=\"80%\"></img>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This customer bought lots of children's gifts and some holiday items. \n\nLook at the descriptions of the recommended products to see if they are in the same categories.\n\n<div class=\"alert alert-block alert-info\">Note: The ALS algorithm uses some randomness, so the recommendations you get might be different from these.</div>", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "stockItems = sqlContext.sql(\"select distinct stockCode, description from retailPurchases\")\nstockItems.registerTempTable(\"stockItems\")\n\nquery = \"\"\"\nselect \n    predictions.*,\n    stockItems.description\nfrom\n    predictions\ninner join stockItems on\n    predictions.stockCode = stockItems.stockCode\norder by predictions.prediction desc\nlimit 10\n\"\"\"\nsqlContext.sql(query).toPandas()", 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stockCode</th>\n      <th>custId</th>\n      <th>prediction</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21242</td>\n      <td>15544</td>\n      <td>0.537044</td>\n      <td>RED RETROSPOT PLATE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22090</td>\n      <td>15544</td>\n      <td>0.519475</td>\n      <td>PAPER BUNTING RETROSPOT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22326</td>\n      <td>15544</td>\n      <td>0.509296</td>\n      <td>ROUND SNACK BOXES SET OF4 WOODLAND</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21987</td>\n      <td>15544</td>\n      <td>0.494924</td>\n      <td>PACK OF 6 SKULL PAPER CUPS</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21122</td>\n      <td>15544</td>\n      <td>0.487287</td>\n      <td>SET/10 PINK POLKADOT PARTY CANDLES</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>21559</td>\n      <td>15544</td>\n      <td>0.486284</td>\n      <td>STRAWBERRY LUNCH BOX WITH CUTLERY</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21124</td>\n      <td>15544</td>\n      <td>0.472611</td>\n      <td>SET/10 BLUE POLKADOT PARTY CANDLES</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>22417</td>\n      <td>15544</td>\n      <td>0.465296</td>\n      <td>PACK OF 60 SPACEBOY CAKE CASES</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>47590</td>\n      <td>15544</td>\n      <td>0.464939</td>\n      <td>BLUE HAPPY BIRTHDAY BUNTING</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>47590</td>\n      <td>15544</td>\n      <td>0.464939</td>\n      <td>PINK HAPPY BIRTHDAY BUNTING</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   stockCode  custId  prediction                          description\n0      21242   15544    0.537044                 RED RETROSPOT PLATE \n1      22090   15544    0.519475              PAPER BUNTING RETROSPOT\n2      22326   15544    0.509296  ROUND SNACK BOXES SET OF4 WOODLAND \n3      21987   15544    0.494924           PACK OF 6 SKULL PAPER CUPS\n4      21122   15544    0.487287   SET/10 PINK POLKADOT PARTY CANDLES\n5      21559   15544    0.486284    STRAWBERRY LUNCH BOX WITH CUTLERY\n6      21124   15544    0.472611   SET/10 BLUE POLKADOT PARTY CANDLES\n7      22417   15544    0.465296       PACK OF 60 SPACEBOY CAKE CASES\n8      47590   15544    0.464939          BLUE HAPPY BIRTHDAY BUNTING\n9      47590   15544    0.464939          PINK HAPPY BIRTHDAY BUNTING"
                    }, 
                    "execution_count": 18, 
                    "metadata": {}
                }
            ], 
            "execution_count": 18
        }, 
        {
            "source": "The recommended products look pretty similar to the purchased products, and, in some cases, are actually the same. Your model works!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"summary\"></a>\n## 7. Summary and next steps\nYou created a predictive model that makes product recommendations for customers and verified that it works.\n\nDig deeper:\n - <a href=\"http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\" target=\"_blank\" rel=\"noopener noreferrer\">Collaborative Filtering</a>\n - <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\" rel=\"noopener noreferrer\">Spark Machine Learning Library (MLlib) Guide</a>\n - <a href=\"http://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">Spark Python API Docs</a>\n\n\n### Authors\n**Carlo Appugliese** is a Spark and Hadoop evangelist at IBM.<br>\n**Braden Callahan** is a Big Data Technical Specialist for IBM.<br>\n**Ross Lewis** is a Big Data Technical Sales Specialist for IBM.<br>\n**Mokhtar Kandil** is a World Wide Big Data Technical Specialist for IBM.\n\n\n## Data citation\nDaqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17).\n\nChen, D. (2012). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<hr>\nCopyright &copy; IBM Corp. 2017-2019. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "<div style=\"background:#F5F7FA; height:100px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Want to do more?</span><span style=\"border: 1px solid #3d70b2;padding: 15px;float:right;margin-right:40px; color:#3d70b2; \"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n<span style=\"color:#5A6872;\"> Try out this notebook with your free trial of IBM Watson Studio.</span>\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}