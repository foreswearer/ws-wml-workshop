{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "################################################################################\n#\n# Licensed Materials - Property of IBM\n# (C) Copyright IBM Corp. 2019\n# US Government Users Restricted Rights - Use, duplication disclosure restricted\n# by GSA ADP Schedule Contract with IBM Corp.\n#\n################################################################################\n", 
            "cell_type": "code", 
            "execution_count": 1, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "## IBM AutoAI Auto-Generated Notebook v1.10.2\n### Representing Pipeline: P3 from run e0fc6dc9-fc8d-4d38-8f90-bf200bb6465f\n\n**Note**: Notebook code generated using AutoAI will execute successfully.  If code is modified or reordered, there is no guarantee it will successfully execute, please read our documentation for more information https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .\n\nBefore modifying the pipeline or trying to re-fit the pipeline, consider:\nThe notebook converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\nThe known_values_list is passed by reference and populated with categorical values during fit of the preprocessing pipeline.  Delete its members before re-fitting.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 1. Set Up", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import sklearn\ntry:\n    import xgboost\nexcept:\n    print('xgboost, if needed, will be installed and imported later')\ntry:\n    import lightgbm\nexcept:\n    print('lightgbm, if needed, will be installed and imported later')\nfrom sklearn.cluster import FeatureAgglomeration\nimport numpy\nfrom numpy import nan, dtype, mean\nimport autoai_libs\nfrom autoai_libs.sklearn.custom_scorers import CustomScorers\nimport sklearn.ensemble\nfrom autoai_libs.cognito.transforms.transform_utils import TExtras, FC\nfrom autoai_libs.transformers.exportable import *\nfrom autoai_libs.utils.exportable_utils import *\nfrom sklearn.pipeline import Pipeline\nknown_values_list=[]\n", 
            "cell_type": "code", 
            "execution_count": 2, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "lightgbm, if needed, will be installed and imported later\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "### 2. Compose Pipeline", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#Load pipeline while attempting to automatically import modules and install missing packages as necessary.\nretries = 0\nsuccessful = False\nwhile retries < 10 and not successful:\n    retries += 1\n    try:\n        #\n        # composing steps for toplevel Pipeline\n        #\n        _input_metadata = {'run_uid': 'e0fc6dc9-fc8d-4d38-8f90-bf200bb6465f', 'pn': 'P3', 'data_source': '', 'target_label_name': 'CHURN', 'learning_type': 'classification', 'optimization_metric': 'roc_auc', 'random_state': 33, 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'data_provenance': {'input_data': [{'id': '1', 'type': 's3', 'connection': {'access_key_id': 'c1801b1f3cf74d7a9681d7d6fb1a5945', 'secret_access_key': '5a2091aadf65397fc79ad06c0c95c3227bbf8793fbb47324', 'endpoint_url': 'https://s3-api.us-geo.objectstorage.softlayer.net'}, 'location': {'type': 's3', 'bucket': 'taller-donotdelete-pr-oig7dhr92xtgem', 'path': 'customer_churn.csv'}}]}}\n        steps=[]\n        #\n        # composing steps for preprocessor Pipeline\n        #\n        preprocessor__input_metadata = None\n        preprocessor_steps=[]\n        #\n        # composing steps for preprocessor_features FeatureUnion\n        #\n        preprocessor_features_transformer_list=[]\n        #\n        # composing steps for preprocessor_features_categorical Pipeline\n        #\n        preprocessor_features_categorical__input_metadata = None\n        preprocessor_features_categorical_steps=[]\n        preprocessor_features_categorical_steps.append(('cat_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[0, 1, 2, 4, 9, 10, 11, 12, 14])))\n        preprocessor_features_categorical_steps.append(('cat_compress_strings', autoai_libs.transformers.exportable.CompressStrings(activate_flag=True,\n                dtypes_list=['char_str', 'char_str', 'float_int_num', 'char_str', 'float_int_num', 'char_str', 'char_str', 'char_str', 'float_int_num'],\n                missing_values_reference_list=['', '-', '?', nan],\n                misslist_list=[[], [], [], [], [], [], [], [], []])))\n        preprocessor_features_categorical_steps.append(('cat_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n        preprocessor_features_categorical_steps.append(('cat_unknown_replacer', autoai_libs.transformers.exportable.NumpyReplaceUnknownValues(filling_values=nan,\n                     filling_values_list=[nan, nan, 100001, nan, 100001, nan, nan, nan, 100001],\n                     known_values_list=known_values_list,\n                     missing_values_reference_list=['', '-', '?', nan])))\n        preprocessor_features_categorical_steps.append(('boolean2float_transformer', autoai_libs.transformers.exportable.boolean2float(activate_flag=True)))\n        preprocessor_features_categorical_steps.append(('cat_imputer', autoai_libs.transformers.exportable.CatImputer(activate_flag=True, missing_values=nan,\n              sklearn_version_family='20', strategy='most_frequent')))\n        preprocessor_features_categorical_steps.append(('cat_encoder', autoai_libs.transformers.exportable.CatEncoder(activate_flag=True, categories='auto',\n              dtype=numpy.float64, encoding='ordinal',\n              handle_unknown='error', sklearn_version_family='20')))\n        preprocessor_features_categorical_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n        # assembling preprocessor_features_categorical_ Pipeline\n        preprocessor_features_categorical_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_categorical_steps)\n        preprocessor_features_transformer_list.append(('categorical', preprocessor_features_categorical_pipeline))\n        #\n        # composing steps for preprocessor_features_numeric Pipeline\n        #\n        preprocessor_features_numeric__input_metadata = None\n        preprocessor_features_numeric_steps=[]\n        preprocessor_features_numeric_steps.append(('num_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[3, 5, 6, 7, 8, 13])))\n        preprocessor_features_numeric_steps.append(('num_floatstr2float_transformer', autoai_libs.transformers.exportable.FloatStr2Float(activate_flag=True,\n                dtypes_list=['float_num', 'float_num', 'float_num', 'float_num', 'float_num', 'float_num'],\n                missing_values_reference_list=[])))\n        preprocessor_features_numeric_steps.append(('num_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n        preprocessor_features_numeric_steps.append(('num_imputer', autoai_libs.transformers.exportable.NumImputer(activate_flag=True, missing_values=nan, strategy='median')))\n        preprocessor_features_numeric_steps.append(('num_scaler', autoai_libs.transformers.exportable.OptStandardScaler(num_scaler_copy=None, num_scaler_with_mean=None,\n                 num_scaler_with_std=None, use_scaler_flag=False)))\n        preprocessor_features_numeric_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n        # assembling preprocessor_features_numeric_ Pipeline\n        preprocessor_features_numeric_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_numeric_steps)\n        preprocessor_features_transformer_list.append(('numeric', preprocessor_features_numeric_pipeline))\n        # assembling preprocessor_features_ FeatureUnion\n        preprocessor_features_pipeline = sklearn.pipeline.FeatureUnion(transformer_list=preprocessor_features_transformer_list)\n        preprocessor_steps.append(('features', preprocessor_features_pipeline))\n        preprocessor_steps.append(('permuter', autoai_libs.transformers.exportable.NumpyPermuteArray(axis=0,\n                 permutation_indices=[0, 1, 2, 4, 9, 10, 11, 12, 14, 3, 5, 6, 7, 8, 13])))\n        # assembling preprocessor_ Pipeline\n        preprocessor_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_steps)\n        steps.append(('preprocessor', preprocessor_pipeline))\n        #\n        # composing steps for cognito Pipeline\n        #\n        cognito__input_metadata = None\n        cognito_steps=[]\n        cognito_steps.append(('0', autoai_libs.cognito.transforms.transform_utils.TA2(fun = numpy.add, name = 'sum', datatypes1 = ['intc', 'intp', 'int_', 'uint8', 'uint16', 'uint32', 'uint64', 'int8', 'int16', 'int32', 'int64', 'short', 'long', 'longlong', 'float16', 'float32', 'float64'], feat_constraints1 = [FC.is_not_categorical], datatypes2 = ['intc', 'intp', 'int_', 'uint8', 'uint16', 'uint32', 'uint64', 'int8', 'int16', 'int32', 'int64', 'short', 'long', 'longlong', 'float16', 'float32', 'float64'], feat_constraints2 = [FC.is_not_categorical], tgraph = None, apply_all = True, col_names = ['Gender', 'Status', 'Children', 'Est Income', 'Car Owner', 'Age', 'LongDistance', 'International', 'Local', 'Dropped', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype', 'Usage', 'RatePlan'], col_dtypes = [dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')])))\n        cognito_steps.append(('1', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 15), additional_col_count_to_keep = 15, ptype = 'classification')))\n        cognito_steps.append(('2', autoai_libs.cognito.transforms.transform_utils.TA1(fun = numpy.tan, name = 'tan', datatypes = ['float'], feat_constraints = [FC.is_not_categorical], tgraph = None, apply_all = True, col_names = ['Gender', 'Status', 'Children', 'Est Income', 'Car Owner', 'Age', 'LongDistance', 'International', 'Local', 'Dropped', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype', 'Usage', 'RatePlan', 'sum(Est Income__Age)', 'sum(Est Income__LongDistance)', 'sum(Est Income__International)', 'sum(Est Income__Local)', 'sum(Est Income__Usage)', 'sum(Age__Est Income)', 'sum(LongDistance__Est Income)', 'sum(LongDistance__International)', 'sum(LongDistance__Local)', 'sum(LongDistance__Usage)', 'sum(International__Est Income)', 'sum(International__LongDistance)', 'sum(Local__Est Income)', 'sum(Usage__Est Income)', 'sum(Usage__LongDistance)'], col_dtypes = [dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')])))\n        cognito_steps.append(('3', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 15), additional_col_count_to_keep = 15, ptype = 'classification')))\n        cognito_steps.append(('4', autoai_libs.cognito.transforms.transform_utils.TAM(tans_class=sklearn.cluster.hierarchical.FeatureAgglomeration(affinity = 'euclidean', compute_full_tree = 'auto', connectivity = None, linkage = 'ward', memory = None, n_clusters = 2, pooling_func = mean), name = 'featureagglomeration', tgraph = None, apply_all = True, col_names = ['Gender', 'Status', 'Children', 'Est Income', 'Car Owner', 'Age', 'LongDistance', 'International', 'Local', 'Dropped', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype', 'Usage', 'RatePlan', 'sum(Est Income__Age)', 'sum(Est Income__LongDistance)', 'sum(Est Income__International)', 'sum(Est Income__Local)', 'sum(Est Income__Usage)', 'sum(Age__Est Income)', 'sum(LongDistance__Est Income)', 'sum(LongDistance__International)', 'sum(LongDistance__Usage)', 'sum(International__Est Income)', 'sum(International__LongDistance)', 'sum(Local__Est Income)', 'sum(Usage__Est Income)', 'sum(Usage__LongDistance)', 'tan(LongDistance)'], col_dtypes = [dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')])))\n        cognito_steps.append(('5', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 15), additional_col_count_to_keep = 15, ptype = 'classification')))\n        # assembling cognito_ Pipeline\n        cognito_pipeline = sklearn.pipeline.Pipeline(steps=cognito_steps)\n        steps.append(('cognito', cognito_pipeline))\n        steps.append(('estimator', lightgbm.sklearn.LGBMClassifier(boosting_type='gbdt', class_weight='balanced',\n                colsample_bytree=1.0, importance_type='split', learning_rate=0.1,\n                max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n                min_split_gain=0.0, n_estimators=100, n_jobs=1, num_leaves=31,\n                objective=None, random_state=33, reg_alpha=0.0, reg_lambda=0.0,\n                silent=True, subsample=1.0, subsample_for_bin=200000,\n                subsample_freq=0)))\n        # assembling  Pipeline\n        pipeline = sklearn.pipeline.Pipeline(steps=steps)\n\n        successful = True\n    except Exception as e:\n        estr = str(e)\n        if estr.startswith('name ') and estr.endswith(' is not defined'):\n            try:\n                import importlib\n                module_name = estr.split(\"'\")[1]\n                module = importlib.import_module(module_name)\n                globals().update({module_name: module})\n            except Exception as import_failure:\n                print('import of ' + module_name + ' failed with: ' + str(import_failure))\n                import subprocess\n                print('attempting pip install of ' + module_name)\n                process = subprocess.Popen('pip install ' + module_name, shell=True)\n                process.wait()\n                try:\n                    print('re-attempting import of ' + module_name)\n                    module = importlib.import_module(module_name)\n                    globals().update({module_name: module})\n                    print('import successful for ' + module_name)\n                except Exception as import_or_installation_failure:\n                    print('failure installing and/or importing ' + module_name + ' error was: ' + str(\n                        import_or_installation_failure))\n                    raise (ModuleNotFoundError('Missing package in environment for ' + module_name +\n                        '? Try import and/or pip install manually?'))\n        else:\n            raise(e)\nif successful:\n    print('Pipeline successfully instantiated')\nelse:\n    raise (ModuleNotFoundError('Remaining missing imports/packages in environment? Retry cell and/or try pip install manually?'))\n", 
            "cell_type": "code", 
            "execution_count": 3, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "import of lightgbm failed with: No module named 'lightgbm'\nattempting pip install of lightgbm\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "re-attempting import of lightgbm\nimport successful for lightgbm\nPipeline successfully instantiated\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\ndata_source='replace_with_path_and_csv_filename'\ntarget_label_name = _input_metadata['target_label_name']\nlearning_type = _input_metadata['learning_type']\noptimization_metric = _input_metadata['optimization_metric']\nrandom_state = _input_metadata['random_state']\ncv_num_folds = _input_metadata['cv_num_folds']\nholdout_fraction = _input_metadata['holdout_fraction']\ndata_provenance = _input_metadata['data_provenance']\n", 
            "cell_type": "code", 
            "execution_count": 4, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "import pandas as pd\ndf = None\nif type(data_provenance) is str:\n    try:\n        df = pd.read_csv(data_provenance) # your data file name here\n    except Exception as e:\n        print(e)\nif df is None:\n    try:\n        data_location = data_provenance['input_data'][0]\n        print('data_location '+ str(data_location))\n        import boto3\n        session = boto3.session.Session()\n        cos = session.client(\n            service_name='s3',\n            aws_access_key_id=data_location['connection']['access_key_id'],\n            aws_secret_access_key=data_location['connection']['secret_access_key'],\n            endpoint_url=data_location['connection']['endpoint_url'],\n            verify=False\n        )\n        local_path = data_location['location']['path']\n        print('local_path ' + str(local_path))\n        cos.download_file(data_location['location']['bucket'],\n                     data_location['location']['path'],\n                     local_path)\n        df = pd.read_csv(local_path) # your data file name here\n    except Exception as e:\n        print(e)\nif df is None:\n    raise(ValueError('need location or credential information to read dataframe from COS'))\n", 
            "cell_type": "code", 
            "execution_count": 5, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "data_location {'id': '1', 'type': 's3', 'connection': {'access_key_id': 'c1801b1f3cf74d7a9681d7d6fb1a5945', 'secret_access_key': '5a2091aadf65397fc79ad06c0c95c3227bbf8793fbb47324', 'endpoint_url': 'https://s3-api.us-geo.objectstorage.softlayer.net'}, 'location': {'type': 's3', 'bucket': 'taller-donotdelete-pr-oig7dhr92xtgem', 'path': 'customer_churn.csv'}}\nlocal_path customer_churn.csv\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n  InsecureRequestWarning)\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "target = target_label_name # your target name here\ndf_y = df[target]\n", 
            "cell_type": "code", 
            "execution_count": 6, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "df_X = df.drop(columns=[target])", 
            "cell_type": "code", 
            "execution_count": 7, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.model_selection import train_test_split", 
            "cell_type": "code", 
            "execution_count": 8, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "# until the problem type is available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\nfrom sklearn.utils.multiclass import type_of_target\nif type_of_target(df_y.values) in ['multiclass', 'binary']:\n    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\nelse:\n    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state)\n", 
            "cell_type": "code", 
            "execution_count": 9, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "pipeline.fit(X, y)\n", 
            "cell_type": "code", 
            "execution_count": 10, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Pipeline(memory=None,\n     steps=[('preprocessor', Pipeline(memory=None,\n     steps=[('features', FeatureUnion(n_jobs=None,\n       transformer_list=[('categorical', Pipeline(memory=None,\n     steps=[('cat_column_selector', NumpyColumnSelector(columns=[0, 1, 2, 4, 9, 10, 11, 12, 14])), ('cat_compress_strings', CompressStrings(...ambda=0.0,\n        silent=True, subsample=1.0, subsample_for_bin=200000,\n        subsample_freq=0))])"
                    }, 
                    "execution_count": 10
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "y_pred = pipeline.predict(X_holdout)\n", 
            "cell_type": "code", 
            "execution_count": 11, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "print(y_holdout)\nprint(y_pred)", 
            "cell_type": "code", 
            "execution_count": 12, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "['T' 'F' 'F' 'T' 'F' 'T' 'F' 'T' 'F' 'F' 'F' 'F' 'F' 'T' 'F' 'T' 'T' 'F'\n 'F' 'F' 'T' 'F' 'F' 'T' 'T' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'T' 'F' 'F' 'F'\n 'T' 'F' 'F' 'T' 'T' 'F' 'F' 'T' 'T' 'T' 'F' 'F' 'T' 'T' 'F' 'F' 'F' 'F'\n 'F' 'T' 'T' 'T' 'F' 'T' 'F' 'T' 'F' 'T' 'F' 'F' 'F' 'F' 'T' 'F' 'T' 'T'\n 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'T' 'T' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n 'F' 'T' 'F' 'T' 'T' 'F' 'T' 'T' 'F' 'T' 'T' 'T' 'F' 'F' 'F' 'T' 'F' 'F'\n 'T' 'T' 'T' 'T' 'T' 'F' 'F' 'F' 'T' 'T' 'F' 'F' 'F' 'F' 'F' 'T' 'F' 'T'\n 'F' 'F' 'F' 'F' 'F' 'T' 'F' 'F' 'T' 'F' 'F' 'F' 'F' 'T' 'F' 'F' 'F' 'T'\n 'F' 'F' 'T' 'T' 'F' 'F' 'T' 'F' 'T' 'T' 'T' 'T' 'F' 'T' 'T' 'T' 'T' 'F'\n 'F' 'T' 'T' 'T' 'F' 'T' 'T' 'F' 'T' 'F' 'T' 'F' 'T' 'F' 'T' 'F' 'T' 'T'\n 'T' 'T' 'F' 'T' 'T' 'T' 'F' 'F' 'F' 'F' 'T' 'F' 'T' 'F' 'F' 'F' 'F' 'T'\n 'F' 'F' 'T' 'F' 'F' 'F' 'T' 'F' 'F']\n['T' 'F' 'F' 'T' 'F' 'T' 'F' 'T' 'F' 'F' 'F' 'F' 'F' 'T' 'F' 'T' 'T' 'F'\n 'F' 'F' 'T' 'F' 'F' 'T' 'T' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'T' 'F' 'F' 'F'\n 'T' 'F' 'F' 'T' 'T' 'F' 'F' 'T' 'T' 'T' 'F' 'F' 'T' 'T' 'F' 'F' 'F' 'F'\n 'F' 'T' 'T' 'T' 'F' 'T' 'F' 'T' 'F' 'T' 'F' 'F' 'F' 'F' 'T' 'F' 'T' 'T'\n 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'T' 'T' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F'\n 'F' 'T' 'F' 'T' 'T' 'F' 'T' 'T' 'F' 'T' 'T' 'T' 'F' 'F' 'F' 'T' 'F' 'F'\n 'T' 'T' 'T' 'T' 'T' 'F' 'F' 'F' 'T' 'F' 'F' 'F' 'F' 'F' 'F' 'T' 'F' 'T'\n 'F' 'F' 'F' 'F' 'F' 'T' 'F' 'F' 'T' 'F' 'F' 'F' 'T' 'T' 'F' 'F' 'F' 'T'\n 'F' 'F' 'T' 'T' 'T' 'F' 'T' 'F' 'T' 'T' 'T' 'T' 'F' 'T' 'T' 'T' 'T' 'F'\n 'F' 'T' 'T' 'T' 'F' 'T' 'T' 'F' 'T' 'F' 'T' 'F' 'T' 'F' 'T' 'F' 'T' 'T'\n 'T' 'T' 'F' 'T' 'T' 'T' 'F' 'F' 'F' 'F' 'T' 'T' 'T' 'F' 'F' 'F' 'F' 'T'\n 'F' 'F' 'T' 'F' 'F' 'F' 'T' 'F' 'F']\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.metrics import get_scorer\nscorer = get_scorer(optimization_metric)\nscorer(pipeline, X_holdout, y_holdout)\n", 
            "cell_type": "code", 
            "execution_count": 13, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9887727448703059"
                    }, 
                    "execution_count": 13
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "#delete CONTENTS of known_values_list before refitting, cloning or cross_validate-ing the pipeline, or previous values will be used.\nfor i in range(len(known_values_list)):\n    del known_values_list[0]", 
            "cell_type": "code", 
            "execution_count": 14, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "from sklearn.model_selection import cross_validate\ncv_results = cross_validate(pipeline, X, y, scoring={optimization_metric:scorer})\nimport numpy as np\nnp.mean(cv_results['test_' + optimization_metric])\n", 
            "cell_type": "code", 
            "execution_count": 15, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n  warnings.warn(CV_WARNING, FutureWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n  warnings.warn(msg, category=DeprecationWarning)\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9968416370301837"
                    }, 
                    "execution_count": 15
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "cv_results", 
            "cell_type": "code", 
            "execution_count": 16, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n  warnings.warn(*warn_args, **warn_kwargs)\n"
                }, 
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "{'fit_time': array([0.46716142, 0.47672248, 0.48002553]),\n 'score_time': array([0.04535294, 0.04322672, 0.04352379]),\n 'test_roc_auc': array([0.99886149, 0.99282038, 0.99884304]),\n 'train_roc_auc': array([1., 1., 1.])}"
                    }, 
                    "execution_count": 16
                }
            ], 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}